{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommender System (Rating out of 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../../../Deep_Learning_Extended/Boltzmann_Machines/\"\n",
    "MOVIES_PATH = BASE_PATH + \"ml-1m/movies.dat\"\n",
    "USERS_PATH = BASE_PATH + \"ml-1m/users.dat\"\n",
    "RATINGS_PATH = BASE_PATH + \"ml-1m/ratings.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(MOVIES_PATH, sep=\"::\", header=None, engine='python', \n",
    "                    encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(USERS_PATH, sep=\"::\", header=None, engine='python', \n",
    "                    encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(RATINGS_PATH, sep=\"::\", header=None, engine='python', \n",
    "                    encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = BASE_PATH + \"ml-100k/u1.base\"\n",
    "TEST_PATH = BASE_PATH + \"ml-100k/u1.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_df = pd.read_csv(TRAIN_PATH, delimiter='\\t')\n",
    "training_set = np.array(training_set_df, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = pd.read_csv(TEST_PATH, delimiter='\\t')\n",
    "test_set = np.array(training_set_df, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_users  = int(max(max(training_set[:, 0]),max(test_set[:, 0])))\n",
    "nb_movies = int(max(max(training_set[:, 1]),max(test_set[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_matrix(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies  = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = convert_to_matrix(training_set)\n",
    "test_set = convert_to_matrix(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked AutoEncoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 \t Loss: 1.77183198928833\n",
      "Epoch: 2/200 \t Loss: 1.0967309474945068\n",
      "Epoch: 3/200 \t Loss: 1.0532041788101196\n",
      "Epoch: 4/200 \t Loss: 1.0383158922195435\n",
      "Epoch: 5/200 \t Loss: 1.0309909582138062\n",
      "Epoch: 6/200 \t Loss: 1.0264800786972046\n",
      "Epoch: 7/200 \t Loss: 1.0239206552505493\n",
      "Epoch: 8/200 \t Loss: 1.0218594074249268\n",
      "Epoch: 9/200 \t Loss: 1.020734190940857\n",
      "Epoch: 10/200 \t Loss: 1.0196418762207031\n",
      "Epoch: 11/200 \t Loss: 1.0188077688217163\n",
      "Epoch: 12/200 \t Loss: 1.018248438835144\n",
      "Epoch: 13/200 \t Loss: 1.017877459526062\n",
      "Epoch: 14/200 \t Loss: 1.0175063610076904\n",
      "Epoch: 15/200 \t Loss: 1.0172780752182007\n",
      "Epoch: 16/200 \t Loss: 1.0169247388839722\n",
      "Epoch: 17/200 \t Loss: 1.016385793685913\n",
      "Epoch: 18/200 \t Loss: 1.0165770053863525\n",
      "Epoch: 19/200 \t Loss: 1.0164834260940552\n",
      "Epoch: 20/200 \t Loss: 1.0161021947860718\n",
      "Epoch: 21/200 \t Loss: 1.0159554481506348\n",
      "Epoch: 22/200 \t Loss: 1.0158191919326782\n",
      "Epoch: 23/200 \t Loss: 1.0158555507659912\n",
      "Epoch: 24/200 \t Loss: 1.0158730745315552\n",
      "Epoch: 25/200 \t Loss: 1.015711784362793\n",
      "Epoch: 26/200 \t Loss: 1.0155450105667114\n",
      "Epoch: 27/200 \t Loss: 1.0154938697814941\n",
      "Epoch: 28/200 \t Loss: 1.0148581266403198\n",
      "Epoch: 29/200 \t Loss: 1.0133665800094604\n",
      "Epoch: 30/200 \t Loss: 1.0116277933120728\n",
      "Epoch: 31/200 \t Loss: 1.0096100568771362\n",
      "Epoch: 32/200 \t Loss: 1.0099730491638184\n",
      "Epoch: 33/200 \t Loss: 1.0057767629623413\n",
      "Epoch: 34/200 \t Loss: 1.0053993463516235\n",
      "Epoch: 35/200 \t Loss: 1.0014036893844604\n",
      "Epoch: 36/200 \t Loss: 0.9989477396011353\n",
      "Epoch: 37/200 \t Loss: 0.9974657893180847\n",
      "Epoch: 38/200 \t Loss: 0.9950169324874878\n",
      "Epoch: 39/200 \t Loss: 0.992840051651001\n",
      "Epoch: 40/200 \t Loss: 0.9923148155212402\n",
      "Epoch: 41/200 \t Loss: 0.9924232959747314\n",
      "Epoch: 42/200 \t Loss: 0.9892174005508423\n",
      "Epoch: 43/200 \t Loss: 0.9842289686203003\n",
      "Epoch: 44/200 \t Loss: 0.9827056527137756\n",
      "Epoch: 45/200 \t Loss: 0.9791980981826782\n",
      "Epoch: 46/200 \t Loss: 0.9807366728782654\n",
      "Epoch: 47/200 \t Loss: 0.977435827255249\n",
      "Epoch: 48/200 \t Loss: 0.9799282550811768\n",
      "Epoch: 49/200 \t Loss: 0.972990095615387\n",
      "Epoch: 50/200 \t Loss: 0.9756695032119751\n",
      "Epoch: 51/200 \t Loss: 0.9826297163963318\n",
      "Epoch: 52/200 \t Loss: 0.9825456738471985\n",
      "Epoch: 53/200 \t Loss: 0.9783950448036194\n",
      "Epoch: 54/200 \t Loss: 0.9792699217796326\n",
      "Epoch: 55/200 \t Loss: 0.9780910611152649\n",
      "Epoch: 56/200 \t Loss: 0.9758661389350891\n",
      "Epoch: 57/200 \t Loss: 0.9741384387016296\n",
      "Epoch: 58/200 \t Loss: 0.9726457595825195\n",
      "Epoch: 59/200 \t Loss: 0.9666573405265808\n",
      "Epoch: 60/200 \t Loss: 0.967848539352417\n",
      "Epoch: 61/200 \t Loss: 0.9670350551605225\n",
      "Epoch: 62/200 \t Loss: 0.965071439743042\n",
      "Epoch: 63/200 \t Loss: 0.9632626175880432\n",
      "Epoch: 64/200 \t Loss: 0.9638014435768127\n",
      "Epoch: 65/200 \t Loss: 0.9626967310905457\n",
      "Epoch: 66/200 \t Loss: 0.9636269807815552\n",
      "Epoch: 67/200 \t Loss: 0.9659982919692993\n",
      "Epoch: 68/200 \t Loss: 0.9677958488464355\n",
      "Epoch: 69/200 \t Loss: 0.960415244102478\n",
      "Epoch: 70/200 \t Loss: 0.9626779556274414\n",
      "Epoch: 71/200 \t Loss: 0.9591352939605713\n",
      "Epoch: 72/200 \t Loss: 0.957679808139801\n",
      "Epoch: 73/200 \t Loss: 0.9561699628829956\n",
      "Epoch: 74/200 \t Loss: 0.954695999622345\n",
      "Epoch: 75/200 \t Loss: 0.9538069367408752\n",
      "Epoch: 76/200 \t Loss: 0.9538633227348328\n",
      "Epoch: 77/200 \t Loss: 0.9529048800468445\n",
      "Epoch: 78/200 \t Loss: 0.954127311706543\n",
      "Epoch: 79/200 \t Loss: 0.951458215713501\n",
      "Epoch: 80/200 \t Loss: 0.9516446590423584\n",
      "Epoch: 81/200 \t Loss: 0.9492433667182922\n",
      "Epoch: 82/200 \t Loss: 0.9486618041992188\n",
      "Epoch: 83/200 \t Loss: 0.9478623270988464\n",
      "Epoch: 84/200 \t Loss: 0.949220597743988\n",
      "Epoch: 85/200 \t Loss: 0.9462610483169556\n",
      "Epoch: 86/200 \t Loss: 0.9478254914283752\n",
      "Epoch: 87/200 \t Loss: 0.945232093334198\n",
      "Epoch: 88/200 \t Loss: 0.9479984641075134\n",
      "Epoch: 89/200 \t Loss: 0.9440066814422607\n",
      "Epoch: 90/200 \t Loss: 0.9471595287322998\n",
      "Epoch: 91/200 \t Loss: 0.9439873695373535\n",
      "Epoch: 92/200 \t Loss: 0.9421420097351074\n",
      "Epoch: 93/200 \t Loss: 0.9420346617698669\n",
      "Epoch: 94/200 \t Loss: 0.941660463809967\n",
      "Epoch: 95/200 \t Loss: 0.940793514251709\n",
      "Epoch: 96/200 \t Loss: 0.9415158033370972\n",
      "Epoch: 97/200 \t Loss: 0.941554069519043\n",
      "Epoch: 98/200 \t Loss: 0.9472554922103882\n",
      "Epoch: 99/200 \t Loss: 0.9496943354606628\n",
      "Epoch: 100/200 \t Loss: 0.9483355283737183\n",
      "Epoch: 101/200 \t Loss: 0.9436841607093811\n",
      "Epoch: 102/200 \t Loss: 0.9449748992919922\n",
      "Epoch: 103/200 \t Loss: 0.9426841139793396\n",
      "Epoch: 104/200 \t Loss: 0.9423618316650391\n",
      "Epoch: 105/200 \t Loss: 0.9421672224998474\n",
      "Epoch: 106/200 \t Loss: 0.9415544867515564\n",
      "Epoch: 107/200 \t Loss: 0.9400568604469299\n",
      "Epoch: 108/200 \t Loss: 0.940776526927948\n",
      "Epoch: 109/200 \t Loss: 0.939551055431366\n",
      "Epoch: 110/200 \t Loss: 0.9401332139968872\n",
      "Epoch: 111/200 \t Loss: 0.939041256904602\n",
      "Epoch: 112/200 \t Loss: 0.9390987157821655\n",
      "Epoch: 113/200 \t Loss: 0.9377337098121643\n",
      "Epoch: 114/200 \t Loss: 0.9375378489494324\n",
      "Epoch: 115/200 \t Loss: 0.9374948740005493\n",
      "Epoch: 116/200 \t Loss: 0.9378187656402588\n",
      "Epoch: 117/200 \t Loss: 0.9359970092773438\n",
      "Epoch: 118/200 \t Loss: 0.9361844658851624\n",
      "Epoch: 119/200 \t Loss: 0.9361822605133057\n",
      "Epoch: 120/200 \t Loss: 0.9362507462501526\n",
      "Epoch: 121/200 \t Loss: 0.9360800981521606\n",
      "Epoch: 122/200 \t Loss: 0.9352403283119202\n",
      "Epoch: 123/200 \t Loss: 0.934059202671051\n",
      "Epoch: 124/200 \t Loss: 0.9345637559890747\n",
      "Epoch: 125/200 \t Loss: 0.9338444471359253\n",
      "Epoch: 126/200 \t Loss: 0.9337811470031738\n",
      "Epoch: 127/200 \t Loss: 0.9327219128608704\n",
      "Epoch: 128/200 \t Loss: 0.9328283667564392\n",
      "Epoch: 129/200 \t Loss: 0.9322452545166016\n",
      "Epoch: 130/200 \t Loss: 0.9332699775695801\n",
      "Epoch: 131/200 \t Loss: 0.9310848712921143\n",
      "Epoch: 132/200 \t Loss: 0.9326087832450867\n",
      "Epoch: 133/200 \t Loss: 0.931873619556427\n",
      "Epoch: 134/200 \t Loss: 0.9310981631278992\n",
      "Epoch: 135/200 \t Loss: 0.9304182529449463\n",
      "Epoch: 136/200 \t Loss: 0.9309752583503723\n",
      "Epoch: 137/200 \t Loss: 0.9304657578468323\n",
      "Epoch: 138/200 \t Loss: 0.9301416873931885\n",
      "Epoch: 139/200 \t Loss: 0.929841935634613\n",
      "Epoch: 140/200 \t Loss: 0.9299291372299194\n",
      "Epoch: 141/200 \t Loss: 0.9290907979011536\n",
      "Epoch: 142/200 \t Loss: 0.9291201233863831\n",
      "Epoch: 143/200 \t Loss: 0.9285035729408264\n",
      "Epoch: 144/200 \t Loss: 0.9289377331733704\n",
      "Epoch: 145/200 \t Loss: 0.9281057119369507\n",
      "Epoch: 146/200 \t Loss: 0.9287235736846924\n",
      "Epoch: 147/200 \t Loss: 0.9282045960426331\n",
      "Epoch: 148/200 \t Loss: 0.9284577965736389\n",
      "Epoch: 149/200 \t Loss: 0.9271972179412842\n",
      "Epoch: 150/200 \t Loss: 0.9274507164955139\n",
      "Epoch: 151/200 \t Loss: 0.9297804236412048\n",
      "Epoch: 152/200 \t Loss: 0.9296179413795471\n",
      "Epoch: 153/200 \t Loss: 0.928195059299469\n",
      "Epoch: 154/200 \t Loss: 0.9272860884666443\n",
      "Epoch: 155/200 \t Loss: 0.9263080358505249\n",
      "Epoch: 156/200 \t Loss: 0.9271105527877808\n",
      "Epoch: 157/200 \t Loss: 0.9256215691566467\n",
      "Epoch: 158/200 \t Loss: 0.925998330116272\n",
      "Epoch: 159/200 \t Loss: 0.9253373742103577\n",
      "Epoch: 160/200 \t Loss: 0.9261525869369507\n",
      "Epoch: 161/200 \t Loss: 0.9247665405273438\n",
      "Epoch: 162/200 \t Loss: 0.9257689714431763\n",
      "Epoch: 163/200 \t Loss: 0.9244376420974731\n",
      "Epoch: 164/200 \t Loss: 0.9252135753631592\n",
      "Epoch: 165/200 \t Loss: 0.9240514636039734\n",
      "Epoch: 166/200 \t Loss: 0.9248587489128113\n",
      "Epoch: 167/200 \t Loss: 0.9233852624893188\n",
      "Epoch: 168/200 \t Loss: 0.9245093464851379\n",
      "Epoch: 169/200 \t Loss: 0.924156129360199\n",
      "Epoch: 170/200 \t Loss: 0.9241489171981812\n",
      "Epoch: 171/200 \t Loss: 0.9228248596191406\n",
      "Epoch: 172/200 \t Loss: 0.9245761632919312\n",
      "Epoch: 173/200 \t Loss: 0.9225019216537476\n",
      "Epoch: 174/200 \t Loss: 0.923608124256134\n",
      "Epoch: 175/200 \t Loss: 0.9220249056816101\n",
      "Epoch: 176/200 \t Loss: 0.9231062531471252\n",
      "Epoch: 177/200 \t Loss: 0.9221031665802002\n",
      "Epoch: 178/200 \t Loss: 0.9228174686431885\n",
      "Epoch: 179/200 \t Loss: 0.9215216040611267\n",
      "Epoch: 180/200 \t Loss: 0.9225714802742004\n",
      "Epoch: 181/200 \t Loss: 0.9217017292976379\n",
      "Epoch: 182/200 \t Loss: 0.9229757785797119\n",
      "Epoch: 183/200 \t Loss: 0.9212964177131653\n",
      "Epoch: 184/200 \t Loss: 0.9221835136413574\n",
      "Epoch: 185/200 \t Loss: 0.9210106134414673\n",
      "Epoch: 186/200 \t Loss: 0.9214335680007935\n",
      "Epoch: 187/200 \t Loss: 0.9205058813095093\n",
      "Epoch: 188/200 \t Loss: 0.9218365550041199\n",
      "Epoch: 189/200 \t Loss: 0.9209166169166565\n",
      "Epoch: 190/200 \t Loss: 0.9212872982025146\n",
      "Epoch: 191/200 \t Loss: 0.9199507832527161\n",
      "Epoch: 192/200 \t Loss: 0.920630931854248\n",
      "Epoch: 193/200 \t Loss: 0.9210265278816223\n",
      "Epoch: 194/200 \t Loss: 0.9227480888366699\n",
      "Epoch: 195/200 \t Loss: 0.920656144618988\n",
      "Epoch: 196/200 \t Loss: 0.9207987189292908\n",
      "Epoch: 197/200 \t Loss: 0.9197420477867126\n",
      "Epoch: 198/200 \t Loss: 0.9203895926475525\n",
      "Epoch: 199/200 \t Loss: 0.918949544429779\n",
      "Epoch: 200/200 \t Loss: 0.9195156097412109\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = 0\n",
    "    counter = 0.\n",
    "    for id_user in range(0, nb_users):\n",
    "        input_data = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input_data.clone()\n",
    "        if (torch.sum(target.data > 0) > 0):\n",
    "            output = sae(input_data)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data * mean_corrector)\n",
    "            counter += 1.\n",
    "            optimizer.step()\n",
    "    print(\"Epoch: {}/{} \\t Loss: {}\".format(epoch, epochs, train_loss/counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9236008524894714\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "counter = 0.\n",
    "for id_user in range(0, nb_users):\n",
    "    input_data = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    if (torch.sum(target.data > 0) > 0):\n",
    "        output = sae(input_data)\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data * mean_corrector)\n",
    "        counter += 1.\n",
    "        optimizer.step()\n",
    "print(\"Test Loss: {}\".format(test_loss/counter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
